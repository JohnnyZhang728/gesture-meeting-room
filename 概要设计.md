#### 引言

##### 编写目的

本文档的编写主要用于阐述非接触式手势识别会议系统的总体设计和总体实现方案。

##### 背景

随着人工智能技术的发展，高水平人机协同正在成为主流的生产和服务方式，提高着各行各业的生产效率，形成新的产业形态。本项目便聚焦于变革传统会议控制系统的交互方式，引入一种全新的非接触式人机交互方式——动态手势操控，从而提升会议过程中各种操作的效率同时方便多人协同。

##### 系统简介

###### 系统名称

基于手势识别的会议控制系统（图片处理子系统）

###### 系统任务

建立一个可以识别动态手势、由动态手势控制的会议系统图片处理子系统，解决传统会议系统不便于实时协作的问题

###### 系统功能 

包括视频采集功能、动态手势识别功能和操作控制功能

#### 系统设计

基于动态手势识别算法的端到端会议控制系统的系统架构设计如图：

![](https://github.com/JohnnyZhang728/gesture-meeting-room/blob/main/imgs/5.1.png)

设计图主要包含了展示层，业务逻辑层，数据加载层和相关的开发环境平台等几个关键的模块，接下来，将会对各个模块进行进一步地介绍。

##### 开发环境平台

在GUI构建方面，主要利用的是PyQt框架，它本质上是一个由 python和Qt库混合而成的开发工具库，其出色的跨平台性成为了诸多开发者选择的原因，这其中，QtCore模块较为核心，主要负责构建数据集和网络模型的加载模块，另外，系统本身多处利用了 OpenCV视觉库的相关特性，该函数库可以提供多种高级别层次的API供相应的业务逻辑进行调用。深度学习框架方面主要釆用了TensorFlow，该框架使用简单，整体架构清晰，并且还提供了多种算法以供模型优化，方便梳理模型参数和进一步的调试。

##### 数据加载层

这里主要是加载实现动态手势识别算法所需的训练好的模型。

动态手势识别不同于静态手势识别，我们需要根据不同动态手势的独特时序特征来进行分类。为此我们决定以动态手势每时每刻的手部关键点坐标为时序特征，并以此构造样本集训练机器学习分类器，而为了获取手部关键点坐标，我们决定基于深度学习方法训练手部目标检测模型以及手部21关键点检测模型。

动态手势识别算法模型的结构图如下：

![](https://github.com/JohnnyZhang728/gesture-meeting-room/blob/main/imgs/5.2.png)

##### 业务逻辑层

主要包括视频采集模块、动态手势识别模块和图片操作模块。视频采集模块通过CV2库调用用户的电脑摄像头进行拍摄，动态手势识别模块调用底层加载的动态手势识别模型处理视频数据进行识别，图片操作模块封装了手势对应的操作功能函数，会根据识别出的手势类型完成相应图片操作。

##### 展示层

包括视频界面和图片操作界面，将展示视频实时处理的结果以及图片操作的效果。

#### 系统流程设计

在基于手势识别的会议控制系统中，动态手势的检测与识别尤其重要，而我们基于深度学习、机器学习技术将三个模型结合起来组成pipeline进行实现，为此设计了如下流程：

![](https://github.com/JohnnyZhang728/gesture-meeting-room/blob/main/imgs/5.3.png)

首先通过电脑摄像头实时采集视频数据，然后调用手部目标检测模型实时获取视频中的手位置，然后将获取到的手部位置图片裁剪下来传入手部21关键点检测回归模型实时获取视频中手的21个关键点的坐标信息，然后将坐标信息实时传入机器学习分类器，计算类型概率，如果超过某种预定动态手势类型的阈值则输出该手势类型为最终识别结果，最后将分类识别的结果作为控制信号调用对应的控制函数实现对应的控制操作。